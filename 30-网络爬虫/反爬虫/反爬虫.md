# 反爬虫

## 1. User-Agent 检测

网站可以检测请求的 User-Agent,如果检测到请求来自爬虫,可以拒绝响应或返回错误页面。所以爬虫需要配置浏览器的 User-Agent 字符串来模拟浏览器请求。

## 2. IP 黑名单

网站可以检测请求来源 IP,如果来自知名爬虫 IP 段,可以拒绝响应或返回错误页面。所以爬虫需要使用代理 IP 来避开黑名单。

## 3. 验证码

网站可以在某些请求响应中返回验证码,要求用户输入验证码才继续操作。这样可以防止自动爬虫访问。爬虫需要结合 OpenCV 等工具自动识别验证码,或者使用人工识别验证码。

## 4. 登录验证

对于登录后才能访问的页面,网站会检验 Cookie 或其他登录信息来校验登录状态,这个也是防止爬虫的一个方法。爬虫需要找到网站的登录接口,并通过解析响应获取登录后的 Cookie 等信息,然后在后续请求中带上该信息来达到登录验证。

## 5. 请求频率限制

网站可以检测同一个 IP 或用户在一定时间内的请求次数,如果超过某个限制,则拒绝响应或返回验证码要求验证。爬虫需要控制请求频率,设置随机的时间间隔来避开此限制。

## 6. 页面结构检查

网站可以检测页面结构或内容是否被非法解析或复制,一旦检测到可以采取技术手段阻止爬虫。爬虫在解析和处理响应页面时需要避免触发此类检测机制。

---

## 打开开发者工具，无限 debugger

```js
(function anonymous() {
  debugger;
});
```

直接行号处右键一律不在此处暂停

## 字体反爬

## 控制台清空

```js
// 取消清空方法
console._c = console.clear;
console.clear = function () {
  return;
};

// 此时日志还会一直出现，需要取消日志输出。

console._l = console.log;
console.log = function () {
  return;
};
```

## 类 JSFUCK 加密反爬
